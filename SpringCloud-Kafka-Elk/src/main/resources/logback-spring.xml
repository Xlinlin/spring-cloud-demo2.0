<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true">
    <include
            resource="org/springframework/boot/logging/logback/defaults.xml"/>

    <!-- logback加载优先于application.yml文件，因此需要从bootstrap.yml文件中读取spring.application.name信息 -->
    <springProperty scope="context" name="springAppName"
                    source="spring.application.name"/>

    <!-- spring bootstrap配置文件中读取 logtash服务信息 -->
    <springProperty scope="context" name="logtashServer"
                    source="spring.logstash.server"/>

    <!-- kafka配置 -->
    <springProperty scope="context" name="kafkaServer"
                    source="spring.logstash.kafka-servers"/>
    <springProperty scope="context" name="kafkaLogTopic"
                    source="spring.logstash.logs-topic"/>

    <!-- 日志在工程中的输出位置 -->
    <property name="LOG_FILE"
              value="${BUILD_FOLDER:-build}/${springAppName}"/>

    <!-- 控制台的日志输出样式 -->
    <property name="CONSOLE_LOG_PATTERN"
              value="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr([${springAppName:-},%X{X-B3-TraceId:-},%X{X-Span-Export:-}]){yellow} %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"/>

    <!-- 控制台Appender -->
    <appender name="console"
              class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>${CONSOLE_LOG_PATTERN}</pattern>
            <charset>utf8</charset>
        </encoder>
    </appender>

    <!-- 通过Tcp Socket输出到logstash服务端 -->
    <appender name="logstash-tcp"
              class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>${logtashServer}</destination>
        <!-- 调整适当大小，解决丢包问题。单位：B -->
        <queueSize>1048576</queueSize>
        <encoder
                class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <timestamp/>
                <pattern>
                    <pattern>
                        {
                        "level":"%level",
                        "service": "${springAppName:-}",
                        "trace": "%X{X-B3-TraceId:-}",
                        "span": "%X{X-B3-SpanId:-}",
                        "stack_trace": "%exception{10}",
                        "req_id": "%X{reqId}",
                        "elapsed_time": "#asLong{%X{elapsedTime}}",
                        "pid": "${PID:-}",
                        "thread": "%thread",
                        "class": "%logger{40}",
                        "method_name":
                        "%method",
                        "message": "%message"
                        }
                    </pattern>
                </pattern>
            </providers>
        </encoder>
    </appender>

    <!-- 发送kafka -->
    <appender name="kafkaAppender"
              class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <encoder
                class="com.github.danielwegener.logback.kafka.encoding.LayoutKafkaMessageEncoder">
            <layout class="net.logstash.logback.layout.LogstashLayout">
                <includeContext>true</includeContext>
                <includeCallerData>true</includeCallerData>
                <includeMdc>true</includeMdc>
                <!-- 自定义属性 -->
                <!-- <customFields>{"serverName":"${springAppName}"}</customFields> -->
                <fieldNames
                        class="net.logstash.logback.fieldnames.ShortenedFieldNames"/>
            </layout>
            <charset>UTF-8</charset>
        </encoder>
        <!--kafka topic 需要与配置文件里面的topic一致 否则kafka会沉默并鄙视你 -->
        <topic>${kafkaLogTopic}</topic>
        <!-- 路由分区策略，可依据HostName、ContextName、ThreadName、LoggerName，默认NoKey -->
        <!-- <keyingStrategy -->
        <!-- class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"
            /> -->
        <!-- 交付策略 一种熔断策略(连接不是上默认发送到控制台/AsynchronousDeliveryStrategy)，一种阻塞策略(直到发送成功/BlockingDeliveryStrategy) -->
        <deliveryStrategy
                class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>
        <!-- 开启后所有不能快速通过网络抵达kafka集群的消息都会被分发到appender -->
        <producerConfig>block.on.buffer.full=false</producerConfig>
        <!-- kafka连接地址 -->
        <producerConfig>bootstrap.servers=${kafkaServer}</producerConfig>

    </appender>

    <!-- 使用异步来记录日志信息，不能阻塞主业务流程-->
    <appender name="async" class="ch.qos.logback.classic.AsyncAppender">
        <discardingThreshold>0</discardingThreshold>
        <queueSize>1000</queueSize>
        <includeCallerData>true</includeCallerData>
        <appender-ref ref="kafkaAppender"/>
    </appender>

    <!-- <logger name="org.springframework.web" level="ERROR" /> -->
    <logger name="org.mongodb.driver" level="ERROR"/>
    <logger name="org.apache.http" level="ERROR"/>
    <logger name="com.netflix.discovery" level="ERROR"/>
    <logger name="org.apache.tomcat" level="info"/>

    <!-- 仅针对member包的日志输出 -->
    <logger name="com.purcotton.omni.member" level="info"
            additivity="false">
        <!-- 到logstash中 -->
        <!-- <appender-ref ref="logstash-tcp" /> -->
        <!-- 异步输出到kafka，不能阻塞流程 -->
        <appender-ref ref="async"/>
        <!--<appender-ref ref="console"/>-->
    </logger>

    <root level="info">
        <appender-ref ref="console"/>
    </root>
</configuration>